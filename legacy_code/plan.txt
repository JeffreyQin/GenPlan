Map encoding

Since all room locations are known beforehand, location of rooms and walls need not to be part of state representation in POMCP
We assign an unique integer index to represent each room (note their coordinates don't matter in our case given a well-defined generator)


POMCP definition

- Each state is encoded by (r1, c1, r2, c2)
  (r1, c1) is exit location
  (r2, c2) is agent position

- Actions are encoded by enums
  1 (up/north)
  2 (right/east)
  3 (down/south)
  4 (left/west)

- An observation obtained should entail rooms that become visible as a result of action just taken
  i.e. a set containing the integer indices of all newly visible rooms

- In original POMCP, history is a sequence of actions and subsequent observation
  Our history will entail all rooms that have become visible in the past
  i.e. a set containing integer indices of all visible/known rooms


for observation and history, we need a set-like data structure that supports equality checking (ideally (O(1)) time), such as a string encoding or tuple

Pseudocode

A state s entails
       current agent position as (r1, c1)
       exit position as (r2, c2)
       a set of all observed open cell (each encoded as pair (r,c))
Actions are encoded in enum
       1 (up)
       2 (right)
       3 (down)
       4 (left)
An observation o entails all open cells that have been observed so far
      i.e. a set of coordinate pairs (r, c)
      note that in our maze search task, observation is unique given state
A history h is an alternating sequence of [a, o, a, o, a, o, ...]
       this means that any observation in h would be a superset of any previous observation in h, as new rooms become observed

class Node
  - children: list[Node] #children[1] will be taking action 1, #children[2] will be taking action 2...
  - num_visited: int (number of times this node was visited)
  - value: float #(value of the history or current obs)
  - observation: sum 2d array being the map (or a set)
  - beliefs: set #set of beliefs


#start by initializing a root
root = Node(intial_belief, initial_value = 0, num_visited = 0)
tree = set(observations, agent_pos) #s

algorithm Search(root:Node):
  while(either some iteration or some time): #keep repeating this a large number of times
    state = random.choice(root.beliefs) randomly pick a state
    simulate(state, root, depth)

  end while

  best_action = children.index(max(children[0].value, children[1].value etc))

  return best_action
end procedure

algorithm Simulate(state, node, depth)
  if(discount^depth < epislon) then #discount factor is too small so we just cut it off
    return 0
  end if

  if not node.observation in tree:
    T.append(node.observation)
    for i in range(1,5): #1-4 possible actions inclusive do
      node.children.append(Node(num_visited = 0, value = 0, beliefs = Null))
    end for
    return Rollout(node, state, depth)
  end if

  action = argmax(node.children.value + c * sqrt(log(node.num_visited)/node.children[i].num_visited)) #sorry its so messy

  new_node, reward = generate(node, action)
  R = r + discount_r * Simulate(new_node, depth + 1)
  
  node.num_visited += 1
  node.children[action].num_visited += 1
  node.children[action].value = node.children[action].value + (R - node.children[action].value)/node.children[action].num_visited
  return R
end procedure


procedure Rollout(node, state, depth):
  if(discount^depth < epislon) then #discount factor is too small so we just cut it off
    return 0
  end if
  action = random(1,4) #uniform distribution sample an action
  new_node, reward = generate(node, action)
  return r + discount_r * Rollout(new_node, state, depth + 1)
end procedure
